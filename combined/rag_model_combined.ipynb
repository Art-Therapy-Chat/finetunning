{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000fa332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from textwrap import dedent\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "import json\n",
    "\n",
    "print(\"✅ 라이브러리 import 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50e52b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트 로드 완료! 총 길이: 52778\n",
      "총 339개 청크 생성 완료\n",
      "\n",
      "[기타] ﻿\n",
      "------------------------------------------------------------\n",
      "[집] 1. 제목: HTP : 집 집은 일상생활에서의 가정생활, 또는 가족 내에서의 자신에 대한 인식을 나타낸다. 자신의 현실의 모습일 수도 있고, 또는 자신이 바라는 모습, 또는 가족의 생활패턴을 나타낸다.\n",
      "------------------------------------------------------------\n",
      "[기타] 1. 지붕 지붕의 핵심은 머리로 상상할 수 있는 생각을 나타낸다.\n",
      "------------------------------------------------------------\n",
      "[기타] ■ 과도하게 큰 지붕을 그린다 자신만의 환상을 가지고 있다 사회생활을 피하며 자신만의 상상에 몰두한다\n",
      "------------------------------------------------------------\n",
      "[기타] ■ 지붕이 그림의 전부인 것 마냥 그린다 상상과 공상을 통해 일상생활을 유지한다 학대를 받았을 가능성이 존재한다. 단 이것이 학대를 나타내는 지표로 사용되어서는 안 된다. 현실적으로 통용되지 않는 정신적인 망상을 할 수 있다 망상을 통해 일상생활을 도피하려는 시도일 수 있다\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_core.documents import Document  # ✅ 수정\n",
    "\n",
    "# ✅ 1. txt 파일 불러오기\n",
    "with open(r\"C:\\Users\\helen\\Desktop\\kt cloud tech up\\basic_project\\HTP_DATA\\브런치 집_나무_사람 해석 수정본 (1).txt\", \"r\", encoding=\"utf-8\") as f:  # ✅ 경로 수정\n",
    "    text = f.read()\n",
    "\n",
    "print(\"텍스트 로드 완료! 총 길이:\", len(text))\n",
    "\n",
    "# ✅ 2. 카테고리 자동 감지 함수\n",
    "def detect_category(text):\n",
    "    if \"집\" in text:\n",
    "        return \"집\"\n",
    "    elif \"나무\" in text:\n",
    "        return \"나무\"\n",
    "    elif \"사람\" in text:\n",
    "        return \"사람\"\n",
    "    else:\n",
    "        return \"기타\"\n",
    "\n",
    "# 1. 줄바꿈 제거, 공백 정리\n",
    "text = re.sub(r'\\s+', ' ', text).strip()  # 연속 공백 모두 1칸으로 통일\n",
    "\n",
    "# 2. 번호 기준으로 청크 나누기\n",
    "chunks = re.split(r'(?=\\n?\\d+\\.)', text)\n",
    "chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "\n",
    "# ✅ 4. 각 청크에 카테고리 태깅 + '*' 기준 2차 청킹\n",
    "langchain_docs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # '*', '■' 기호가 있으면 추가로 분리\n",
    "    if '■' in chunk or '*' in chunk:\n",
    "        subchunks = re.split(r'(?=[■*])', chunk)\n",
    "        subchunks = [s.strip() for s in subchunks if s.strip()]\n",
    "        for j, subchunk in enumerate(subchunks):\n",
    "            category = detect_category(subchunk)\n",
    "            langchain_docs.append(\n",
    "                Document(\n",
    "                    page_content=subchunk,\n",
    "                    metadata={\n",
    "                        \"chunk_index\": f\"{i + 1}-{j + 1}\",  # ex: 3-1, 3-2\n",
    "                        \"category\": category\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        category = detect_category(chunk)\n",
    "        langchain_docs.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"chunk_index\": i + 1,\n",
    "                    \"category\": category\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "# ✅ 5. 결과 확인\n",
    "print(f\"총 {len(langchain_docs)}개 청크 생성 완료\\n\")\n",
    "for doc in langchain_docs[:5]:  # 상위 5개만 미리보기\n",
    "    print(f\"[{doc.metadata['category']}] {doc.page_content}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550a3504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 기타\n",
      "2: 집\n",
      "3: 기타\n",
      "4: 기타\n",
      "5: 기타\n",
      "6: 기타\n",
      "7: 기타\n",
      "8: 기타\n",
      "9: 기타\n",
      "10: 기타\n",
      "11: 사람\n",
      "12: 사람\n",
      "13: 기타\n",
      "14: 사람\n",
      "15: 사람\n",
      "16: 기타\n",
      "17: 기타\n",
      "18: 기타\n",
      "19: 집\n",
      "20: 기타\n",
      "21: 기타\n",
      "22: 기타\n",
      "23: 사람\n",
      "24: 기타\n",
      "25: 기타\n",
      "26: 집\n",
      "27: 집\n",
      "28: 집\n",
      "29: 기타\n",
      "30: 집\n",
      "31: 집\n",
      "32: 기타\n",
      "33: 집\n",
      "34: 기타\n",
      "35: 기타\n",
      "36: 사람\n",
      "37: 기타\n",
      "38: 기타\n",
      "39: 기타\n",
      "40: 기타\n",
      "41: 기타\n",
      "42: 기타\n",
      "43: 사람\n",
      "44: 기타\n",
      "45: 기타\n",
      "46: 기타\n",
      "47: 집\n",
      "48: 기타\n",
      "49: 기타\n",
      "50: 기타\n",
      "51: 집\n",
      "52: 기타\n",
      "53: 기타\n",
      "54: 집\n",
      "55: 집\n",
      "56: 집\n",
      "57: 사람\n",
      "58: 집\n",
      "59: 사람\n",
      "60: 집\n",
      "61: 집\n",
      "62: 집\n",
      "63: 집\n",
      "64: 집\n",
      "65: 집\n",
      "66: 집\n",
      "67: 집\n",
      "68: 기타\n",
      "69: 집\n",
      "70: 집\n",
      "71: 집\n",
      "72: 집\n",
      "73: 집\n",
      "74: 기타\n",
      "75: 사람\n",
      "76: 집\n",
      "77: 기타\n",
      "78: 집\n",
      "79: 나무\n",
      "80: 나무\n",
      "81: 나무\n",
      "82: 나무\n",
      "83: 나무\n",
      "84: 나무\n",
      "85: 나무\n",
      "86: 나무\n",
      "87: 나무\n",
      "88: 나무\n",
      "89: 나무\n",
      "90: 나무\n",
      "91: 나무\n",
      "92: 나무\n",
      "93: 나무\n",
      "94: 나무\n",
      "95: 나무\n",
      "96: 나무\n",
      "97: 기타\n",
      "98: 나무\n",
      "99: 사람\n",
      "100: 집\n",
      "101: 기타\n",
      "102: 기타\n",
      "103: 기타\n",
      "104: 기타\n",
      "105: 집\n",
      "106: 기타\n",
      "107: 기타\n",
      "108: 기타\n",
      "109: 나무\n",
      "110: 사람\n",
      "111: 나무\n",
      "112: 집\n",
      "113: 기타\n",
      "114: 기타\n",
      "115: 기타\n",
      "116: 기타\n",
      "117: 기타\n",
      "118: 기타\n",
      "119: 기타\n",
      "120: 기타\n",
      "121: 사람\n",
      "122: 기타\n",
      "123: 기타\n",
      "124: 사람\n",
      "125: 기타\n",
      "126: 기타\n",
      "127: 기타\n",
      "128: 기타\n",
      "129: 기타\n",
      "130: 기타\n",
      "131: 기타\n",
      "132: 기타\n",
      "133: 기타\n",
      "134: 기타\n",
      "135: 기타\n",
      "136: 사람\n",
      "137: 기타\n",
      "138: 기타\n",
      "139: 기타\n",
      "140: 기타\n",
      "141: 집\n",
      "142: 기타\n",
      "143: 사람\n",
      "144: 사람\n",
      "145: 기타\n",
      "146: 기타\n",
      "147: 기타\n",
      "148: 기타\n",
      "149: 사람\n",
      "150: 나무\n",
      "151: 기타\n",
      "152: 기타\n",
      "153: 사람\n",
      "154: 기타\n",
      "155: 사람\n",
      "156: 사람\n",
      "157: 사람\n",
      "158: 기타\n",
      "159: 기타\n",
      "160: 기타\n",
      "161: 나무\n",
      "162: 나무\n",
      "163: 나무\n",
      "164: 나무\n",
      "165: 기타\n",
      "166: 집\n",
      "167: 사람\n",
      "168: 사람\n",
      "169: 사람\n",
      "170: 사람\n",
      "171: 집\n",
      "172: 기타\n",
      "173: 기타\n",
      "174: 기타\n",
      "175: 기타\n",
      "176: 기타\n",
      "177: 기타\n",
      "178: 기타\n",
      "179: 기타\n",
      "180: 기타\n",
      "181: 사람\n",
      "182: 사람\n",
      "183: 기타\n",
      "184: 기타\n",
      "185: 기타\n",
      "186: 집\n",
      "187: 집\n",
      "188: 기타\n",
      "189: 기타\n",
      "190: 기타\n",
      "191: 기타\n",
      "192: 기타\n",
      "193: 기타\n",
      "194: 기타\n",
      "195: 집\n",
      "196: 기타\n",
      "197: 집\n",
      "198: 기타\n",
      "199: 기타\n",
      "200: 사람\n",
      "201: 기타\n",
      "202: 집\n",
      "203: 기타\n",
      "204: 사람\n",
      "205: 사람\n",
      "206: 사람\n",
      "207: 집\n",
      "208: 기타\n",
      "209: 사람\n",
      "210: 기타\n",
      "211: 기타\n",
      "212: 기타\n",
      "213: 사람\n",
      "214: 기타\n",
      "215: 사람\n",
      "216: 사람\n",
      "217: 기타\n",
      "218: 기타\n",
      "219: 사람\n",
      "220: 기타\n",
      "221: 기타\n",
      "222: 기타\n",
      "223: 기타\n",
      "224: 기타\n",
      "225: 기타\n",
      "226: 기타\n",
      "227: 기타\n",
      "228: 사람\n",
      "229: 기타\n",
      "230: 기타\n",
      "231: 기타\n",
      "232: 기타\n",
      "233: 기타\n",
      "234: 기타\n",
      "235: 사람\n",
      "236: 기타\n",
      "237: 기타\n",
      "238: 사람\n",
      "239: 기타\n",
      "240: 사람\n",
      "241: 사람\n",
      "242: 기타\n",
      "243: 기타\n",
      "244: 사람\n",
      "245: 기타\n",
      "246: 사람\n",
      "247: 기타\n",
      "248: 기타\n",
      "249: 기타\n",
      "250: 기타\n",
      "251: 기타\n",
      "252: 기타\n",
      "253: 사람\n",
      "254: 기타\n",
      "255: 기타\n",
      "256: 기타\n",
      "257: 기타\n",
      "258: 기타\n",
      "259: 기타\n",
      "260: 기타\n",
      "261: 기타\n",
      "262: 기타\n",
      "263: 기타\n",
      "264: 집\n",
      "265: 사람\n",
      "266: 기타\n",
      "267: 기타\n",
      "268: 사람\n",
      "269: 사람\n",
      "270: 기타\n",
      "271: 기타\n",
      "272: 기타\n",
      "273: 기타\n",
      "274: 기타\n",
      "275: 기타\n",
      "276: 기타\n",
      "277: 사람\n",
      "278: 사람\n",
      "279: 기타\n",
      "280: 기타\n",
      "281: 기타\n",
      "282: 기타\n",
      "283: 기타\n",
      "284: 기타\n",
      "285: 기타\n",
      "286: 사람\n",
      "287: 기타\n",
      "288: 기타\n",
      "289: 사람\n",
      "290: 기타\n",
      "291: 기타\n",
      "292: 집\n",
      "293: 집\n",
      "294: 집\n",
      "295: 기타\n",
      "296: 기타\n",
      "297: 기타\n",
      "298: 기타\n",
      "299: 기타\n",
      "300: 기타\n",
      "301: 기타\n",
      "302: 기타\n",
      "303: 집\n",
      "304: 기타\n",
      "305: 기타\n",
      "306: 기타\n",
      "307: 집\n",
      "308: 기타\n",
      "309: 사람\n",
      "310: 기타\n",
      "311: 집\n",
      "312: 기타\n",
      "313: 기타\n",
      "314: 기타\n",
      "315: 기타\n",
      "316: 사람\n",
      "317: 기타\n",
      "318: 집\n",
      "319: 기타\n",
      "320: 기타\n",
      "321: 기타\n",
      "322: 집\n",
      "323: 기타\n",
      "324: 기타\n",
      "325: 기타\n",
      "326: 기타\n",
      "327: 기타\n",
      "328: 기타\n",
      "329: 기타\n",
      "330: 기타\n",
      "331: 사람\n",
      "332: 기타\n",
      "333: 기타\n",
      "334: 기타\n",
      "335: 기타\n",
      "336: 기타\n",
      "337: 기타\n",
      "338: 집\n",
      "339: 사람\n"
     ]
    }
   ],
   "source": [
    "# 카테고리가 잘 들어갔는지 확인\n",
    "for i, doc in enumerate(langchain_docs, start=1):\n",
    "    print(f\"{i}: {doc.metadata.get('category', '없음')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13734715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'chunk_index': '13-6', 'category': '기타'}, page_content='■ 둥지가 가늘고 작다 자기 스스로에 대해 모자라고 나쁘다는 느낌을 받는다 자신을 지킬 힘이 없다고 느낀다')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28517a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 수정\n",
    "for i, doc in enumerate(langchain_docs, start=1):\n",
    "    if 0 <= i <= 51:\n",
    "        doc.metadata['category'] = '집'\n",
    "    elif 52 <= i <= 101:\n",
    "        doc.metadata['category'] = '나무'\n",
    "    elif 102 <= i :\n",
    "        doc.metadata['category'] = '사람'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfdaa219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 집\n",
      "1: 집\n",
      "2: 집\n",
      "3: 집\n",
      "4: 집\n",
      "5: 집\n",
      "6: 집\n",
      "7: 집\n",
      "8: 집\n",
      "9: 집\n",
      "10: 집\n",
      "11: 집\n",
      "12: 집\n",
      "13: 집\n",
      "14: 집\n",
      "15: 집\n",
      "16: 집\n",
      "17: 집\n",
      "18: 집\n",
      "19: 집\n",
      "20: 집\n",
      "21: 집\n",
      "22: 집\n",
      "23: 집\n",
      "24: 집\n",
      "25: 집\n",
      "26: 집\n",
      "27: 집\n",
      "28: 집\n",
      "29: 집\n",
      "30: 집\n",
      "31: 집\n",
      "32: 집\n",
      "33: 집\n",
      "34: 집\n",
      "35: 집\n",
      "36: 집\n",
      "37: 집\n",
      "38: 집\n",
      "39: 집\n",
      "40: 집\n",
      "41: 집\n",
      "42: 집\n",
      "43: 집\n",
      "44: 집\n",
      "45: 집\n",
      "46: 집\n",
      "47: 집\n",
      "48: 집\n",
      "49: 집\n",
      "50: 집\n",
      "51: 나무\n",
      "52: 나무\n",
      "53: 나무\n",
      "54: 나무\n",
      "55: 나무\n",
      "56: 나무\n",
      "57: 나무\n",
      "58: 나무\n",
      "59: 나무\n",
      "60: 나무\n",
      "61: 나무\n",
      "62: 나무\n",
      "63: 나무\n",
      "64: 나무\n",
      "65: 나무\n",
      "66: 나무\n",
      "67: 나무\n",
      "68: 나무\n",
      "69: 나무\n",
      "70: 나무\n",
      "71: 나무\n",
      "72: 나무\n",
      "73: 나무\n",
      "74: 나무\n",
      "75: 나무\n",
      "76: 나무\n",
      "77: 나무\n",
      "78: 나무\n",
      "79: 나무\n",
      "80: 나무\n",
      "81: 나무\n",
      "82: 나무\n",
      "83: 나무\n",
      "84: 나무\n",
      "85: 나무\n",
      "86: 나무\n",
      "87: 나무\n",
      "88: 나무\n",
      "89: 나무\n",
      "90: 나무\n",
      "91: 나무\n",
      "92: 나무\n",
      "93: 나무\n",
      "94: 나무\n",
      "95: 나무\n",
      "96: 나무\n",
      "97: 나무\n",
      "98: 나무\n",
      "99: 나무\n",
      "100: 나무\n",
      "101: 사람\n",
      "102: 사람\n",
      "103: 사람\n",
      "104: 사람\n",
      "105: 사람\n",
      "106: 사람\n",
      "107: 사람\n",
      "108: 사람\n",
      "109: 사람\n",
      "110: 사람\n",
      "111: 사람\n",
      "112: 사람\n",
      "113: 사람\n",
      "114: 사람\n",
      "115: 사람\n",
      "116: 사람\n",
      "117: 사람\n",
      "118: 사람\n",
      "119: 사람\n",
      "120: 사람\n",
      "121: 사람\n",
      "122: 사람\n",
      "123: 사람\n",
      "124: 사람\n",
      "125: 사람\n",
      "126: 사람\n",
      "127: 사람\n",
      "128: 사람\n",
      "129: 사람\n",
      "130: 사람\n",
      "131: 사람\n",
      "132: 사람\n",
      "133: 사람\n",
      "134: 사람\n",
      "135: 사람\n",
      "136: 사람\n",
      "137: 사람\n",
      "138: 사람\n",
      "139: 사람\n",
      "140: 사람\n",
      "141: 사람\n",
      "142: 사람\n",
      "143: 사람\n",
      "144: 사람\n",
      "145: 사람\n",
      "146: 사람\n",
      "147: 사람\n",
      "148: 사람\n",
      "149: 사람\n",
      "150: 사람\n",
      "151: 사람\n",
      "152: 사람\n",
      "153: 사람\n",
      "154: 사람\n",
      "155: 사람\n",
      "156: 사람\n",
      "157: 사람\n",
      "158: 사람\n",
      "159: 사람\n",
      "160: 사람\n",
      "161: 사람\n",
      "162: 사람\n",
      "163: 사람\n",
      "164: 사람\n",
      "165: 사람\n",
      "166: 사람\n",
      "167: 사람\n",
      "168: 사람\n",
      "169: 사람\n",
      "170: 사람\n",
      "171: 사람\n",
      "172: 사람\n",
      "173: 사람\n",
      "174: 사람\n",
      "175: 사람\n",
      "176: 사람\n",
      "177: 사람\n",
      "178: 사람\n",
      "179: 사람\n",
      "180: 사람\n",
      "181: 사람\n",
      "182: 사람\n",
      "183: 사람\n",
      "184: 사람\n",
      "185: 사람\n",
      "186: 사람\n",
      "187: 사람\n",
      "188: 사람\n",
      "189: 사람\n",
      "190: 사람\n",
      "191: 사람\n",
      "192: 사람\n",
      "193: 사람\n",
      "194: 사람\n",
      "195: 사람\n",
      "196: 사람\n",
      "197: 사람\n",
      "198: 사람\n",
      "199: 사람\n",
      "200: 사람\n",
      "201: 사람\n",
      "202: 사람\n",
      "203: 사람\n",
      "204: 사람\n",
      "205: 사람\n",
      "206: 사람\n",
      "207: 사람\n",
      "208: 사람\n",
      "209: 사람\n",
      "210: 사람\n",
      "211: 사람\n",
      "212: 사람\n",
      "213: 사람\n",
      "214: 사람\n",
      "215: 사람\n",
      "216: 사람\n",
      "217: 사람\n",
      "218: 사람\n",
      "219: 사람\n",
      "220: 사람\n",
      "221: 사람\n",
      "222: 사람\n",
      "223: 사람\n",
      "224: 사람\n",
      "225: 사람\n",
      "226: 사람\n",
      "227: 사람\n",
      "228: 사람\n",
      "229: 사람\n",
      "230: 사람\n",
      "231: 사람\n",
      "232: 사람\n",
      "233: 사람\n",
      "234: 사람\n",
      "235: 사람\n",
      "236: 사람\n",
      "237: 사람\n",
      "238: 사람\n",
      "239: 사람\n",
      "240: 사람\n",
      "241: 사람\n",
      "242: 사람\n",
      "243: 사람\n",
      "244: 사람\n",
      "245: 사람\n",
      "246: 사람\n",
      "247: 사람\n",
      "248: 사람\n",
      "249: 사람\n",
      "250: 사람\n",
      "251: 사람\n",
      "252: 사람\n",
      "253: 사람\n",
      "254: 사람\n",
      "255: 사람\n",
      "256: 사람\n",
      "257: 사람\n",
      "258: 사람\n",
      "259: 사람\n",
      "260: 사람\n",
      "261: 사람\n",
      "262: 사람\n",
      "263: 사람\n",
      "264: 사람\n",
      "265: 사람\n",
      "266: 사람\n",
      "267: 사람\n",
      "268: 사람\n",
      "269: 사람\n",
      "270: 사람\n",
      "271: 사람\n",
      "272: 사람\n",
      "273: 사람\n",
      "274: 사람\n",
      "275: 사람\n",
      "276: 사람\n",
      "277: 사람\n",
      "278: 사람\n",
      "279: 사람\n",
      "280: 사람\n",
      "281: 사람\n",
      "282: 사람\n",
      "283: 사람\n",
      "284: 사람\n",
      "285: 사람\n",
      "286: 사람\n",
      "287: 사람\n",
      "288: 사람\n",
      "289: 사람\n",
      "290: 사람\n",
      "291: 사람\n",
      "292: 사람\n",
      "293: 사람\n",
      "294: 사람\n",
      "295: 사람\n",
      "296: 사람\n",
      "297: 사람\n",
      "298: 사람\n",
      "299: 사람\n",
      "300: 사람\n",
      "301: 사람\n",
      "302: 사람\n",
      "303: 사람\n",
      "304: 사람\n",
      "305: 사람\n",
      "306: 사람\n",
      "307: 사람\n",
      "308: 사람\n",
      "309: 사람\n",
      "310: 사람\n",
      "311: 사람\n",
      "312: 사람\n",
      "313: 사람\n",
      "314: 사람\n",
      "315: 사람\n",
      "316: 사람\n",
      "317: 사람\n",
      "318: 사람\n",
      "319: 사람\n",
      "320: 사람\n",
      "321: 사람\n",
      "322: 사람\n",
      "323: 사람\n",
      "324: 사람\n",
      "325: 사람\n",
      "326: 사람\n",
      "327: 사람\n",
      "328: 사람\n",
      "329: 사람\n",
      "330: 사람\n",
      "331: 사람\n",
      "332: 사람\n",
      "333: 사람\n",
      "334: 사람\n",
      "335: 사람\n",
      "336: 사람\n",
      "337: 사람\n",
      "338: 사람\n"
     ]
    }
   ],
   "source": [
    "# 수정된 카테고리\n",
    "for i, doc in enumerate(langchain_docs, start=1):\n",
    "    print(f\"{i-1}: {doc.metadata.get('category', '없음')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9388ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 최대 길이 : 4048\n",
      "문서의 최소 길이 : 1\n",
      "문서의 평균 길이 : 151.7669616519174\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 각 문서 길이 계산 (문자 수 기준)\n",
    "doc_lengths = [len(doc.page_content) for doc in langchain_docs]\n",
    "\n",
    "# 문서 최소, 평균 길이 출력\n",
    "print('문서의 최대 길이 :', max(doc_lengths))\n",
    "print('문서의 최소 길이 :', min(doc_lengths))\n",
    "print('문서의 평균 길이 :', sum(doc_lengths) / len(doc_lengths))\n",
    "\n",
    "# 문서 인덱스 (1, 2, 3, …)\n",
    "doc_indices = range(1, len(langchain_docs) + 1)\n",
    "\n",
    "# 막대그래프 그리기\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(doc_indices, doc_lengths, color='cornflowerblue', edgecolor='black')\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Length of Document')\n",
    "plt.title('Document Lengths by Index')\n",
    "plt.xticks(doc_indices)  # 문서 번호 표시\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca0a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "길이가 10 이하인 문서 개수: 6\n"
     ]
    }
   ],
   "source": [
    "doc_lengths_10 = [doc for doc in langchain_docs if len(doc.page_content) < 10]\n",
    "\n",
    "# 몇 개 있는지 출력\n",
    "print(f\"길이가 10 이하인 문서 개수: {len(doc_lengths_10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c1d76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'chunk_index': 1, 'category': '집'}, page_content='\\ufeff'),\n",
       " Document(metadata={'chunk_index': '26-1', 'category': '사람'}, page_content='4. 손과 다리'),\n",
       " Document(metadata={'chunk_index': '27-25', 'category': '사람'}, page_content='* 해부적인 표현'),\n",
       " Document(metadata={'chunk_index': '28-1', 'category': '사람'}, page_content='6. 자세'),\n",
       " Document(metadata={'chunk_index': '29-1', 'category': '사람'}, page_content='7. 의상'),\n",
       " Document(metadata={'chunk_index': '29-2', 'category': '사람'}, page_content='* 옷')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aff20b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'chunk_index': 1, 'category': '집'}, page_content='\\ufeff')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce8ab97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 문서 개수: 333\n"
     ]
    }
   ],
   "source": [
    "# 기존 langchain_docs에서 길이가 10 이하인 문서 제거\n",
    "langchain_docs = [doc for doc in langchain_docs if len(doc.page_content.strip()) > 10]\n",
    "\n",
    "print(f\"학습 데이터 문서 개수: {len(langchain_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc35027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 제목: HTP : 집 집은 일상생활에서의 가정생활, 또는 가족 내에서의 자신에 대한 인식을 나타낸다. 자신의 현실의 모습일 수도 있고, 또는 자신이 바라는 모습, 또는 가족의 생활패턴을 나타낸다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [doc.page_content for doc in langchain_docs]\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21719d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 파인튜닝된 임베딩 모델 로드\n",
    "embedding_model_name = \"HJUNN/bge-m3b-Art-Therapy-embedding-fine-tuning\"\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embedding_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e91e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"텍스트를 임베딩 벡터로 변환\"\"\"\n",
    "    inputs = embed_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = embed_model(**inputs).last_hidden_state[:,0,:]  # [CLS] 토큰\n",
    "        emb = emb / emb.norm(dim=1, keepdim=True)  # 정규화\n",
    "    return emb.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb5b55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper 만들기\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class MyEmbeddings(Embeddings):\n",
    "    def __init__(self, model, tokenizer, device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"리스트[str] -> 리스트[np.ndarray]\"\"\"\n",
    "        return [self.embed_query(t) for t in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            emb = self.model(**inputs).last_hidden_state[:, 0, :]\n",
    "            emb = emb / emb.norm(dim=1, keepdim=True)\n",
    "        return emb.cpu().numpy()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6327db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 벡터 DB 생성 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp\\ipykernel_13992\\3077111381.py:14: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma  # ✅ 수정\n",
    "\n",
    "embeddings = MyEmbeddings(embed_model, embed_tokenizer, device=device)\n",
    "\n",
    "# Chroma DB 생성\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=langchain_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"htp_collection\",\n",
    "    persist_directory=\"./chroma_store\"\n",
    ")\n",
    "\n",
    "# 영구 저장\n",
    "vectorstore.persist()\n",
    "print(\"✅ 벡터 DB 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406eec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "cross_encoder = CrossEncoder('BAAI/bge-reranker-v2-m3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a985607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "\n",
    "class Retriever_with_cross_encoder(BaseRetriever):\n",
    "    vectorstore: Any\n",
    "    crossencoder: Any\n",
    "    k: int\n",
    "    rerank_top_k: int\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"필수 구현 메서드 (언더스코어로 시작)\"\"\"\n",
    "        # 첫번째 검색 결과 k개 (기본값=5)\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "\n",
    "        # pairs = 검색 문서와 검색어의 모든 쌍들\n",
    "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
    "\n",
    "        # 검색 문서와 검색어가 모두 입력으로 사용.\n",
    "        scores = self.crossencoder.predict(pairs)\n",
    "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 점수가 높은 top k만 남긴다. (기본값=2)\n",
    "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ec3fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder_retriever = Retriever_with_cross_encoder(\n",
    "    vectorstore=vectorstore,\n",
    "    crossencoder=cross_encoder,\n",
    "    k=20,\n",
    "    rerank_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fd681c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 쿼리 재생성 모델 정의\n",
    "class MultiQueryGenerator(BaseModel):\n",
    "    queries : List[str] = Field(description= \"사용자 질문에서 추출한 검색 쿼리 목록\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa89e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import template\n",
    "\n",
    "# Hugging Face 모델 기반 쿼리 재작성기\n",
    "class AdvancedQueryRewriter:\n",
    "    def __init__(self, model_name=\"helena29/Qwen2.5_LoRA_for_HTP\"):\n",
    "        \"\"\"\n",
    "        Hugging Face 모델을 사용한 쿼리 재작성기\n",
    "        Args:\n",
    "            model_name: Hugging Face 모델 이름\n",
    "        \"\"\"\n",
    "        print(f\"✅ 쿼리 재작성 모델 로딩 중: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # ✅ device_map 없이 로드하고 수동으로 디바이스 할당\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            trust_remote_code=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"✅ 쿼리 재작성 모델 로딩 완료! Device: {self.device}\")\n",
    "\n",
    "        # 영어 프롬프트 템플릿\n",
    "        self.template = \"\"\"You are an assistant that regenerates search queries based on the user's previous conversations and questions.\n",
    "\n",
    "# Instructions\n",
    "1. Reference all previous queries/retrieved documents/answers in the history below to generate more accurate search queries.\n",
    "2. If the current question is ambiguous or incomplete, use the history to reconstruct a contextually complete query.\n",
    "3. If there is no history or it's not relevant, use only the current question.\n",
    "4. Always generate clear and search-appropriate queries.\n",
    "5. The output should contain only the regenerated query strings. Do not include additional explanations or comments.\n",
    "6. If the current sentence contains multiple attributes, separate each into individual queries.\n",
    "7. Each query should be complete and clear enough to be independently searchable in a vector DB.\n",
    "8. When combined, the separated queries should represent the meaning of the original query.\n",
    "\n",
    "# Input\n",
    "Full conversation history: {history_text}\n",
    "Current question: {current_query}\n",
    "\n",
    "# Output Format\n",
    "You must output in the following JSON format:\n",
    "{{\n",
    "    \"queries\": [\"query1\", \"query2\", ...]\n",
    "}}\n",
    "\n",
    "Example:\n",
    "If the current question is \"What about Seoul? And restaurants?\" and the previous conversation was \"Recommend tourist spots in Korea\",\n",
    "{{\n",
    "    \"queries\": [\"Recommend tourist spots in Seoul\", \"Recommend restaurants in Seoul\"]\n",
    "}}\n",
    "\n",
    "Single query case:\n",
    "{{\n",
    "    \"queries\": [\"Recommend tourist spots in Korea\"]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    def rewrite_query(self, history_text: str, current_query: str) -> List[str]:\n",
    "        if not history_text.strip():\n",
    "            history_text = \"No previous conversation\"\n",
    "\n",
    "        try:\n",
    "            # 프롬프트 구성\n",
    "            prompt = self.template.format(\n",
    "                history_text=history_text,\n",
    "                current_query=current_query\n",
    "            )\n",
    "            \n",
    "            # Qwen 형식으로 포맷팅\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for query rewriting.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # 토큰화 및 생성\n",
    "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=500,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # 디코딩\n",
    "            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "            \n",
    "            # JSON 파싱\n",
    "            try:\n",
    "                import json\n",
    "                import re\n",
    "                \n",
    "                # JSON 부분만 추출\n",
    "                json_match = re.search(r'\\{[^{}]*\"queries\"[^{}]*\\}', response, re.DOTALL)\n",
    "                if json_match:\n",
    "                    response_json = json.loads(json_match.group())\n",
    "                else:\n",
    "                    response_json = json.loads(response)\n",
    "                \n",
    "                if \"queries\" in response_json and isinstance(response_json[\"queries\"], list):\n",
    "                    return response_json[\"queries\"]\n",
    "                else:\n",
    "                    print(f\"Response doesn't contain 'queries' list: {response}\")\n",
    "                    return [current_query]\n",
    "            except (json.JSONDecodeError, Exception) as e:\n",
    "                print(f\"JSON parsing error: {str(e)}\")\n",
    "                print(f\"Model raw response: {response}\")\n",
    "                return [current_query]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during query rewriting: {str(e)}\")\n",
    "            return [current_query]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f381d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 쿼리 검색 클래스 정의\n",
    "class MultiQueryRetriever:\n",
    "    def __init__(self, vectorstore, query_rewriter, **kwargs):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.query_rewriter = query_rewriter\n",
    "        self.history = []\n",
    "\n",
    "    def build_history_text(self) -> str:\n",
    "        text = \"\"\n",
    "        for h in self.history:\n",
    "            text += f\"[QUESTION]\\n{h['user_query']}\\n\"\n",
    "            text += f\"[REWRITTEN QUERIES]\\n{h['rewritten_queries']}\\n\"\n",
    "            text += \"[RETRIEVED DOCS]\\n\"\n",
    "            for d in h[\"retrieved_docs\"]:\n",
    "                text += f\"- {d['content']}\\n\"\n",
    "            text += f\"[ANSWER]\\n{h['final_answer']}\\n\"\n",
    "            text += \"-\"*40 + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    def retrieve(self, query:str, num_docs = 3)-> List[Document]:\n",
    "\n",
    "        history_text = self.build_history_text()\n",
    "\n",
    "        rewritten_queries = self.query_rewriter.rewrite_query(\n",
    "            history_text=history_text,\n",
    "            current_query=query\n",
    "        )\n",
    "\n",
    "        print(f\"원래 쿼리: {query}\")\n",
    "        print(f\"재생성된 쿼리들: {rewritten_queries}\")\n",
    "\n",
    "        # 모든 쿼리에 대해 검색 수행 및 결과 병합\n",
    "        all_docs = []\n",
    "        seen_contents = set()\n",
    "\n",
    "        # 다수의 쿼리가 주어졌을 때 1개씩 검색해본다.\n",
    "        for idx, rewritten_query in enumerate(rewritten_queries):\n",
    "            print(f\"쿼리 {idx+1} : {rewritten_query}\")\n",
    "\n",
    "            #  벡터 검색 수행\n",
    "            docs = self.vectorstore.similarity_search(rewritten_query, k = num_docs)\n",
    "\n",
    "            # 중복 제거하여 문서 추가\n",
    "            for doc in docs:\n",
    "                if doc.page_content not in seen_contents:\n",
    "                    seen_contents.add(doc.page_content)\n",
    "\n",
    "                    #메타데이터에 쿼리 정보 추가\n",
    "                    if not hasattr(doc, \"metadata\") or doc.metadata is None:\n",
    "                        doc.metadata = {}\n",
    "                    doc.metadata['query'] = rewritten_query\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "        print(f\"총 {len(all_docs)}개의 고유 문서를 검색했습니다.\")\n",
    "        return all_docs, rewritten_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b06c0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 응답 생성 클래스 정의 (Hugging Face 업로드된 모델 사용)\n",
    "class AdvancedConversationalRAG:\n",
    "    def __init__(self, vectorstore, model_name=\"helena29/Qwen2.5_LoRA_for_HTP\"):\n",
    "        \"\"\"\n",
    "        Hugging Face에 업로드된 fine-tuned 모델을 사용한 대화형 RAG 시스템\n",
    "        Args:\n",
    "            vectorstore: 벡터 저장소\n",
    "            model_name: Hugging Face 모델 이름 (기본값: helena29/Qwen2.5_LoRA_for_HTP)\n",
    "        \"\"\"\n",
    "        # history에 대화 저장\n",
    "        self.history = []\n",
    "        \n",
    "        # 쿼리 재생성기 (동일한 모델 사용)\n",
    "        self.query_rewriter = AdvancedQueryRewriter(model_name=model_name)\n",
    "        \n",
    "        # 각각의 검색어를 따로 검색한 뒤에 검색결과를 취합하는 멀티쿼리 리트리버\n",
    "        self.retriever = MultiQueryRetriever(vectorstore=vectorstore, query_rewriter=self.query_rewriter)\n",
    "        \n",
    "        # 답변 생성용 모델 로드 (쿼리 재작성기와 같은 모델 재사용)\n",
    "        print(f\"✅ 답변 생성에도 동일 모델 사용: {model_name}\")\n",
    "        self.tokenizer = self.query_rewriter.tokenizer\n",
    "        self.llm = self.query_rewriter.model\n",
    "        self.device = self.query_rewriter.device\n",
    "        print(f\"✅ 모델 설정 완료! Device: {self.device}\")\n",
    "\n",
    "        # 응답 생성을 위한 프롬프트 템플릿 (영어 버전)\n",
    "        self.response_template = \"\"\"You are a professional psychologist specialized in HTP (House-Tree-Person) test interpretation.\n",
    "Your role is to provide clear, professional psychological interpretations based on drawing features.\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Please provide your interpretation based on the following reference information:\n",
    "{context}\n",
    "\n",
    "Guidelines:\n",
    "1. If the user's question contains multiple queries, address each one clearly and separately.\n",
    "2. Base your answer only on the provided information. If information is insufficient, honestly state that you don't know.\n",
    "3. Provide your answer in Korean language.\n",
    "4. If there are original sources in the provided information, cite them appropriately.\n",
    "5. Explain possible psychological meanings in a professional manner.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        \"\"\"Fine-tuned 모델로 응답 생성\"\"\"\n",
    "        # Qwen 형식으로 포맷팅\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional psychologist specialized in HTP test interpretation.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # 토큰화 및 생성 (디바이스 명시적 지정)\n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # ✅ 모든 입력 텐서를 모델과 같은 디바이스로 이동\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 디코딩 (입력 부분 제외)\n",
    "        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "        \n",
    "    def query(self, current_query: str) -> Dict:\n",
    "        # 관련 문서검색\n",
    "        docs, rewritten_queries = self.retriever.retrieve(current_query)\n",
    "\n",
    "        # 문서 내용을 컨텍스트로 변환\n",
    "        if docs:\n",
    "            context = \"\\n\\n\".join([f\"문서 {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "            formatted_prompt = self.response_template.format(query=current_query, context=context)\n",
    "        else:\n",
    "            # 문서 없으면 일반 지식 기반 답변 생성\n",
    "            formatted_prompt = f\"User Question: {current_query}\\n\\nNo documents were retrieved, but please provide an appropriate answer based on your knowledge.\"\n",
    "\n",
    "        # Fine-tuned LLM으로 응답 생성\n",
    "        response = self.generate_response(formatted_prompt)\n",
    "\n",
    "        # 히스토리에 저장\n",
    "        record = {\n",
    "            \"user_query\": current_query,\n",
    "            \"rewritten_queries\": rewritten_queries,\n",
    "            \"retrieved_docs\": [\n",
    "                {\"content\": d.page_content, \"metadata\": d.metadata} for d in docs\n",
    "            ],\n",
    "            \"final_answer\": response\n",
    "        }\n",
    "        self.history.append(record)\n",
    "        self.retriever.history.append(record)\n",
    "\n",
    "        # 결과 반환\n",
    "        return {\n",
    "            \"query\": current_query,\n",
    "            \"result\": response,\n",
    "            \"rewritten_queries\": rewritten_queries,\n",
    "            \"source_documents\": docs\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d6d393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 쿼리 재작성 모델 로딩 중: helena29/Qwen2.5_LoRA_for_HTP\n",
      "✅ 쿼리 재작성 모델 로딩 완료! Device: cuda\n",
      "✅ 답변 생성에도 동일 모델 사용: helena29/Qwen2.5_LoRA_for_HTP\n",
      "✅ 모델 설정 완료! Device: cuda\n",
      "✅ 쿼리 재작성 모델 로딩 완료! Device: cuda\n",
      "✅ 답변 생성에도 동일 모델 사용: helena29/Qwen2.5_LoRA_for_HTP\n",
      "✅ 모델 설정 완료! Device: cuda\n"
     ]
    }
   ],
   "source": [
    "conversational_rag = AdvancedConversationalRAG(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ff5c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "질문 1: 손이 크고, 눈썹이 진한 사람 그림\n",
      "원래 쿼리: 손이 크고, 눈썹이 진한 사람 그림\n",
      "재생성된 쿼리들: ['generate portrait with large hands and dark eyebrows']\n",
      "쿼리 1 : generate portrait with large hands and dark eyebrows\n",
      "원래 쿼리: 손이 크고, 눈썹이 진한 사람 그림\n",
      "재생성된 쿼리들: ['generate portrait with large hands and dark eyebrows']\n",
      "쿼리 1 : generate portrait with large hands and dark eyebrows\n",
      "총 1개의 고유 문서를 검색했습니다.\n",
      "총 1개의 고유 문서를 검색했습니다.\n",
      "답변 1:\n",
      "The presence of an unusually large hand and dark eyebrows suggests a heightened sense of self-awareness and possibly a tendency towards impulsivity or a need for control. This could indicate a person who is highly attuned to their own feelings and experiences, potentially leading to anxiety or difficulty with regulation. The specific characteristics should be further explored within the context of the full HTP assessment to determine if this pattern aligns with any particular psychological profile or condition.\n",
      "답변 1:\n",
      "The presence of an unusually large hand and dark eyebrows suggests a heightened sense of self-awareness and possibly a tendency towards impulsivity or a need for control. This could indicate a person who is highly attuned to their own feelings and experiences, potentially leading to anxiety or difficulty with regulation. The specific characteristics should be further explored within the context of the full HTP assessment to determine if this pattern aligns with any particular psychological profile or condition.\n"
     ]
    }
   ],
   "source": [
    "# 예시 1: 첫 번째 질문 (이전 대화 없음)\n",
    "query1 = \"손이 크고, 눈썹이 진한 사람 그림\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"질문 1: {query1}\")\n",
    "result1 = conversational_rag.query(query1)\n",
    "print(f\"답변 1:\\n{result1['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff1f7fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "질문 2: 손가락을 자세히 나타냈어\n",
      "원래 쿼리: 손가락을 자세히 나타냈어\n",
      "재생성된 쿼리들: ['Generate portrait with large hands and dark eyebrows', 'Retrieved document details']\n",
      "쿼리 1 : Generate portrait with large hands and dark eyebrows\n",
      "쿼리 2 : Retrieved document details\n",
      "총 2개의 고유 문서를 검색했습니다.\n",
      "원래 쿼리: 손가락을 자세히 나타냈어\n",
      "재생성된 쿼리들: ['Generate portrait with large hands and dark eyebrows', 'Retrieved document details']\n",
      "쿼리 1 : Generate portrait with large hands and dark eyebrows\n",
      "쿼리 2 : Retrieved document details\n",
      "총 2개의 고유 문서를 검색했습니다.\n",
      "답변 2:\n",
      "Based on the detailed description of the hand drawn by the user, it appears to indicate an overemphasis on detail and perfectionism. The meticulous depiction of the tree and leaf suggests a tendency towards excessive analysis and possibly anxiety about achieving ideal outcomes. This could be indicative of obsessive-compulsive tendencies or a need for order and control within their environment. Further investigation would be needed to determine if this reflects a genuine personality trait or simply a preference for structured thinking.\n",
      "답변 2:\n",
      "Based on the detailed description of the hand drawn by the user, it appears to indicate an overemphasis on detail and perfectionism. The meticulous depiction of the tree and leaf suggests a tendency towards excessive analysis and possibly anxiety about achieving ideal outcomes. This could be indicative of obsessive-compulsive tendencies or a need for order and control within their environment. Further investigation would be needed to determine if this reflects a genuine personality trait or simply a preference for structured thinking.\n"
     ]
    }
   ],
   "source": [
    "# 예시 2: 두 번째 질문\n",
    "query2 = \"손가락을 자세히 나타냈어\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"질문 2: {query2}\")\n",
    "result2 = conversational_rag.query(query2)\n",
    "print(f\"답변 2:\\n{result2['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82f23381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "질문 3: 창문이 크게 있어\n",
      "원래 쿼리: 창문이 크게 있어\n",
      "재생성된 쿼리들: ['Generate portrait with large windows', 'Retrieve document related to large windows']\n",
      "쿼리 1 : Generate portrait with large windows\n",
      "쿼리 2 : Retrieve document related to large windows\n",
      "총 2개의 고유 문서를 검색했습니다.\n",
      "원래 쿼리: 창문이 크게 있어\n",
      "재생성된 쿼리들: ['Generate portrait with large windows', 'Retrieve document related to large windows']\n",
      "쿼리 1 : Generate portrait with large windows\n",
      "쿼리 2 : Retrieve document related to large windows\n",
      "총 2개의 고유 문서를 검색했습니다.\n",
      "\n",
      "답변 3:\n",
      "The large window depicted in the HTP test suggests an overemphasis on external relationships and social interactions, potentially indicating a need for validation from others or a fear of isolation. This could be indicative of low self-esteem or difficulty forming genuine connections due to a tendency to rely heavily on external validation rather than developing a strong sense of self-worth and autonomy. The document references suggest this may reflect a desire to appear impressive or secure within the eyes of others, possibly stemming from underlying insecurities.\n",
      "\n",
      "==================================================\n",
      "\n",
      "참조 문서들:\n",
      "\n",
      "문서 3:\n",
      "■ 창문을 강조하며 그린다 인간관계에 대한 과한 걱정이 존재한다\n",
      "메타데이터: {'category': '집', 'chunk_index': '6-8', 'query': 'Generate portrait with large windows'}\n",
      "\n",
      "문서 4:\n",
      "■ 집의 크기에 비해 매우 큰 문을 그린다 타인에게 극도로 의존하고 싶은 마음이 존재한다 삶의 판단과 책임을 다른 사람에게 맡겨버리고 싶다는 표현일 수 있다 남들에게 인상적인 모습을 남기고, 멋있는 사람인 것처럼 표현하는 시도일 수 있다\n",
      "메타데이터: {'chunk_index': '5-5', 'category': '집', 'query': 'Retrieve document related to large windows'}\n",
      "\n",
      "답변 3:\n",
      "The large window depicted in the HTP test suggests an overemphasis on external relationships and social interactions, potentially indicating a need for validation from others or a fear of isolation. This could be indicative of low self-esteem or difficulty forming genuine connections due to a tendency to rely heavily on external validation rather than developing a strong sense of self-worth and autonomy. The document references suggest this may reflect a desire to appear impressive or secure within the eyes of others, possibly stemming from underlying insecurities.\n",
      "\n",
      "==================================================\n",
      "\n",
      "참조 문서들:\n",
      "\n",
      "문서 3:\n",
      "■ 창문을 강조하며 그린다 인간관계에 대한 과한 걱정이 존재한다\n",
      "메타데이터: {'category': '집', 'chunk_index': '6-8', 'query': 'Generate portrait with large windows'}\n",
      "\n",
      "문서 4:\n",
      "■ 집의 크기에 비해 매우 큰 문을 그린다 타인에게 극도로 의존하고 싶은 마음이 존재한다 삶의 판단과 책임을 다른 사람에게 맡겨버리고 싶다는 표현일 수 있다 남들에게 인상적인 모습을 남기고, 멋있는 사람인 것처럼 표현하는 시도일 수 있다\n",
      "메타데이터: {'chunk_index': '5-5', 'category': '집', 'query': 'Retrieve document related to large windows'}\n"
     ]
    }
   ],
   "source": [
    "# 예시 3: 세 번째 질문\n",
    "query3 = \"창문이 크게 있어\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"질문 3: {query3}\")\n",
    "result3 = conversational_rag.query(query3)\n",
    "print(f\"\\n답변 3:\\n{result3['result']}\")\n",
    "\n",
    "# 인용한 문서 확인\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n참조 문서들:\")\n",
    "for i, doc in enumerate(result3['source_documents'], 3):\n",
    "    print(f\"\\n문서 {i}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"메타데이터:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14ab4e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_query': '손이 크고, 눈썹이 진한 사람 그림',\n",
       "  'rewritten_queries': ['generate portrait with large hands and dark eyebrows'],\n",
       "  'retrieved_docs': [{'content': '■ 극단적으로 큰 손을 그린다 자신의 무언가가 옳지 않다, 또는 자신이 모자라다는 느낌을 갖는다 충동적으로 행동한다',\n",
       "    'metadata': {'category': '사람',\n",
       "     'chunk_index': '26-17',\n",
       "     'query': 'generate portrait with large hands and dark eyebrows'}}],\n",
       "  'final_answer': 'The presence of an unusually large hand and dark eyebrows suggests a heightened sense of self-awareness and possibly a tendency towards impulsivity or a need for control. This could indicate a person who is highly attuned to their own feelings and experiences, potentially leading to anxiety or difficulty with regulation. The specific characteristics should be further explored within the context of the full HTP assessment to determine if this pattern aligns with any particular psychological profile or condition.'},\n",
       " {'user_query': '손가락을 자세히 나타냈어',\n",
       "  'rewritten_queries': ['Generate portrait with large hands and dark eyebrows',\n",
       "   'Retrieved document details'],\n",
       "  'retrieved_docs': [{'content': '■ 극단적으로 큰 손을 그린다 자신의 무언가가 옳지 않다, 또는 자신이 모자라다는 느낌을 갖는다 충동적으로 행동한다',\n",
       "    'metadata': {'category': '사람',\n",
       "     'chunk_index': '26-17',\n",
       "     'query': 'Generate portrait with large hands and dark eyebrows'}},\n",
       "   {'content': '■ 2차원으로 꼼꼼하고 촘촘하게 묘사된 잎을 그린다 반복적으로 행동 또는 생각을 반복하며 완벽을 추구하는 경향이 있다.',\n",
       "    'metadata': {'chunk_index': '17-7',\n",
       "     'category': '사람',\n",
       "     'query': 'Retrieved document details'}}],\n",
       "  'final_answer': 'Based on the detailed description of the hand drawn by the user, it appears to indicate an overemphasis on detail and perfectionism. The meticulous depiction of the tree and leaf suggests a tendency towards excessive analysis and possibly anxiety about achieving ideal outcomes. This could be indicative of obsessive-compulsive tendencies or a need for order and control within their environment. Further investigation would be needed to determine if this reflects a genuine personality trait or simply a preference for structured thinking.'},\n",
       " {'user_query': '창문이 크게 있어',\n",
       "  'rewritten_queries': ['Generate portrait with large windows',\n",
       "   'Retrieve document related to large windows'],\n",
       "  'retrieved_docs': [{'content': '■ 창문을 강조하며 그린다 인간관계에 대한 과한 걱정이 존재한다',\n",
       "    'metadata': {'category': '집',\n",
       "     'chunk_index': '6-8',\n",
       "     'query': 'Generate portrait with large windows'}},\n",
       "   {'content': '■ 집의 크기에 비해 매우 큰 문을 그린다 타인에게 극도로 의존하고 싶은 마음이 존재한다 삶의 판단과 책임을 다른 사람에게 맡겨버리고 싶다는 표현일 수 있다 남들에게 인상적인 모습을 남기고, 멋있는 사람인 것처럼 표현하는 시도일 수 있다',\n",
       "    'metadata': {'chunk_index': '5-5',\n",
       "     'category': '집',\n",
       "     'query': 'Retrieve document related to large windows'}}],\n",
       "  'final_answer': 'The large window depicted in the HTP test suggests an overemphasis on external relationships and social interactions, potentially indicating a need for validation from others or a fear of isolation. This could be indicative of low self-esteem or difficulty forming genuine connections due to a tendency to rely heavily on external validation rather than developing a strong sense of self-worth and autonomy. The document references suggest this may reflect a desire to appear impressive or secure within the eyes of others, possibly stemming from underlying insecurities.'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "018ee6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 대화 히스토리가 'conversational_rag_history_20251121_101252.json'에 저장되었습니다!\n",
      "총 3개의 대화가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 대화 히스토리를 JSON 파일로 저장\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# 현재 시간을 파일명에 포함\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"conversational_rag_history_{timestamp}.json\"\n",
    "\n",
    "# history를 JSON으로 저장\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(conversational_rag.history, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 대화 히스토리가 '{filename}'에 저장되었습니다!\")\n",
    "print(f\"총 {len(conversational_rag.history)}개의 대화가 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fdfb071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "질문 4: 기차가 지나간다\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m질문 4: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery4\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m result4 \u001b[38;5;241m=\u001b[39m \u001b[43mconversational_rag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m답변 4:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 82\u001b[0m, in \u001b[0;36mAdvancedConversationalRAG.query\u001b[1;34m(self, current_query)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# 관련 문서검색\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     docs, rewritten_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# 문서 내용을 컨텍스트로 변환\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m docs:\n",
      "Cell \u001b[1;32mIn[26], line 24\u001b[0m, in \u001b[0;36mMultiQueryRetriever.retrieve\u001b[1;34m(self, query, num_docs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query:\u001b[38;5;28mstr\u001b[39m, num_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     22\u001b[0m     history_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_history_text()\n\u001b[1;32m---> 24\u001b[0m     rewritten_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_rewriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewrite_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m원래 쿼리: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m재생성된 쿼리들: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewritten_queries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 88\u001b[0m, in \u001b[0;36mAdvancedQueryRewriter.rewrite_query\u001b[1;34m(self, history_text, current_query)\u001b[0m\n\u001b[0;32m     85\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 88\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     90\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m     91\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     92\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     93\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     94\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m     95\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m     96\u001b[0m     )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# 디코딩\u001b[39;00m\n\u001b[0;32m     99\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m][inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   2791\u001b[0m     outputs,\n\u001b[0;32m   2792\u001b[0m     model_kwargs,\n\u001b[0;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2794\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:449\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[0;32m    431\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[0;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    450\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    451\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    452\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    453\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    454\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    455\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    456\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    460\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:384\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[1;32m--> 384\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    385\u001b[0m         hidden_states,\n\u001b[0;32m    386\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask_mapping[decoder_layer\u001b[38;5;241m.\u001b[39mattention_type],\n\u001b[0;32m    387\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    388\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    389\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    390\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    391\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    393\u001b[0m     )\n\u001b[0;32m    395\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[0;32m    397\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    398\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    399\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:234\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    235\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    236\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    237\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    238\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    239\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    240\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    241\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    244\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:153\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    151\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m--> 153\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    154\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    155\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:771\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_input_dtype(x, lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_variant:  \u001b[38;5;66;03m# vanilla LoRA\u001b[39;00m\n\u001b[1;32m--> 771\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m lora_B(lora_A(dropout(x))) \u001b[38;5;241m*\u001b[39m scaling\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_variant[active_adapter]\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    775\u001b[0m         active_adapter\u001b[38;5;241m=\u001b[39mactive_adapter,\n\u001b[0;32m    776\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    777\u001b[0m         result\u001b[38;5;241m=\u001b[39mresult,\n\u001b[0;32m    778\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query4 = \"기차가 지나간다\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"질문 4: {query4}\")\n",
    "result4 = conversational_rag.query(query4)\n",
    "print(f\"\\n답변 4:\\n{result4['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186c842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
