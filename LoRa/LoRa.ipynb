{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc98b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 6: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2b3119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 정보:\n",
      "GPU Name: NVIDIA GeForce RTX 4060\n",
      "GPU Memory: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "# ========== GPU 메모리 확인 ==========\n",
    "print(\"GPU 정보:\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddd9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 설정 ==========\n",
    "@dataclass\n",
    "class Config:\n",
    "    # 모델 설정\n",
    "    model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"  \n",
    "    # model_name: str = \"meta-llama/Llama-2-7b-hf\"  # 7B 모델 (16GB 이상)\n",
    "    \n",
    "    # 데이터 경로\n",
    "    train_data_path: str = \"HTP_data.jsonl\"\n",
    "    \n",
    "    # 출력 경로\n",
    "    output_dir: str = \"./htp_lora_model\"\n",
    "    \n",
    "    # 배치 크기 (RTX 4060 8GB용)\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # 학습 설정\n",
    "    num_train_epochs: int = 10\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.03\n",
    "    \n",
    "    # 텍스트 길이\n",
    "    max_seq_length: int = 512\n",
    "    \n",
    "    # LoRA 설정\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    \n",
    "    # 8-bit 양자화 (메모리 절감)\n",
    "    use_8bit: bool = True\n",
    "    \n",
    "    # 기타\n",
    "    seed: int = 42\n",
    "    fp16: bool = True  # Mixed precision training\n",
    "\n",
    "\n",
    "# ========== 데이터 로딩 ==========\n",
    "def load_jsonl_data(file_path: str) -> list:\n",
    "    \"\"\"JSONL 파일 읽기\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_dataset(data: list, tokenizer, config: Config) -> Dataset:\n",
    "    \"\"\"데이터셋 준비\"\"\"\n",
    "    \n",
    "    def formatting_func(examples):\n",
    "        # input과 output을 합쳐서 학습 데이터 생성\n",
    "        texts = []\n",
    "        for inp, out in zip(examples['input'], examples['output']):\n",
    "            text = f\"HTP 해석 입력: {inp}\\n\\nHTP 해석 출력: {out}\"\n",
    "            texts.append(text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            texts,\n",
    "            max_length=config.max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        # labels 설정 (input_ids와 동일하게)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Dataset으로 변환\n",
    "    dataset = Dataset.from_dict({\n",
    "        'input': [item['input'] for item in data],\n",
    "        'output': [item['output'] for item in data],\n",
    "    })\n",
    "    \n",
    "    # 처리\n",
    "    processed_dataset = dataset.map(\n",
    "        formatting_func,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        remove_columns=['input', 'output'],\n",
    "        desc=\"포매팅 중...\",\n",
    "    )\n",
    "    \n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "# ========== 모델 로딩 및 LoRA 설정 ==========\n",
    "def setup_model_and_tokenizer(config: Config):\n",
    "    \"\"\"모델과 토크나이저 설정\"\"\"\n",
    "    \n",
    "    print(f\"모델 로딩: {config.model_name}\")\n",
    "    \n",
    "    # 토크나이저 로딩\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 모델 로딩 (8-bit 양자화)\n",
    "    if config.use_8bit:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            load_in_8bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        # 8-bit 양자화 모델 준비\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model,\n",
    "            use_gradient_checkpointing=True,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    \n",
    "    # LoRA 설정\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],  # 모델에 따라 수정 필요\n",
    "    )\n",
    "    \n",
    "    # LoRA 적용\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # 학습 가능한 파라미터 확인\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d96d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로딩 중...\n",
      "로드된 샘플 수: 1453\n",
      "모델 로딩: Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,089,536 || all params: 1,544,803,840 || trainable%: 0.0705\n",
      "데이터셋 준비 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9eec23730445c682fca634ff38d133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "포매팅 중...:   0%|          | 0/1453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 샘플: 1307, 검증 샘플: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1640' max='1640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1640/1640 2:27:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.204048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.197915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.195824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>0.196182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.195154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.196050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>0.196372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.196334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\helen\\anaconda3\\envs\\LLMenv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 중...\n",
      "✓ 파인튜닝 완료: ./htp_lora_model\n"
     ]
    }
   ],
   "source": [
    "# ========== 메인 학습 함수 ==========\n",
    "config = Config()\n",
    "\n",
    "# 시드 설정\n",
    "transformers.set_seed(config.seed)\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. 데이터 로드\n",
    "print(\"데이터 로딩 중...\")\n",
    "raw_data = load_jsonl_data(config.train_data_path)\n",
    "print(f\"로드된 샘플 수: {len(raw_data)}\")\n",
    "\n",
    "# 2. 모델, 토크나이저 설정\n",
    "model, tokenizer = setup_model_and_tokenizer(config)\n",
    "\n",
    "# 3. 데이터셋 준비\n",
    "print(\"데이터셋 준비 중...\")\n",
    "dataset = prepare_dataset(raw_data, tokenizer, config)\n",
    "\n",
    "# 학습/검증 분할 (9:1)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=config.seed)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"학습 샘플: {len(train_dataset)}, 검증 샘플: {len(eval_dataset)}\")\n",
    "\n",
    "# 4. 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # 배치 크기\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    \n",
    "    # 에폭\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    \n",
    "    # 학습률\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    \n",
    "    # 옵티마이저\n",
    "    optim=\"paged_adamw_32bit\",  # 8-bit 최적화\n",
    "    \n",
    "    # 저장 및 로깅\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    \n",
    "    # 계산 최적화\n",
    "    fp16=config.fp16,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # 기타\n",
    "    seed=config.seed,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# 5. Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8),\n",
    ")\n",
    "\n",
    "# 6. 학습 시작\n",
    "print(\"학습 시작...\")\n",
    "trainer.train()\n",
    "\n",
    "# 7. 모델 저장\n",
    "print(\"모델 저장 중...\")\n",
    "model.save_pretrained(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "print(f\"✓ 파인튜닝 완료: {config.output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd119469",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# ========== 파인튜닝된 모델 로드 ==========\n",
    "def load_finetuned_model(model_path: str):\n",
    "    \"\"\"파인튜닝된 모델 로드\"\"\"\n",
    "    \n",
    "    # LoRA 모델 로드\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ========== 추론 함수 ==========\n",
    "def generate_htp_interpretation(model, tokenizer, htp_input: str, max_length: int = 256):\n",
    "    \"\"\"HTP 해석 생성\"\"\"\n",
    "    \n",
    "    # 입력 준비\n",
    "    prompt = f\"HTP 해석 입력: {htp_input}\\n\\nHTP 해석 출력:\"\n",
    "    \n",
    "    # 토큰화\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # 디코딩\n",
    "    response = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # 출력 부분만 추출\n",
    "    if \"HTP 해석 출력:\" in response:\n",
    "        response = response.split(\"HTP 해석 출력:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b75e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 사용 예시 ==========\n",
    "# 모델 로드\n",
    "model, tokenizer = load_finetuned_model(\"./htp_lora_model\")\n",
    "\n",
    "# HTP 해석 생성\n",
    "htp_input = \"나무가 크고 가지가 많으며 뿌리가 깊게 표현됨\"\n",
    "\n",
    "interpretation = generate_htp_interpretation(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    htp_input,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "print(f\"입력: {htp_input}\")\n",
    "print(f\"해석: {interpretation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46644f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
