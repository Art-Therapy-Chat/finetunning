{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== GPU ë©”ëª¨ë¦??•ì¸ ==========\n",
    "print(\"GPU ?•ë³´:\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ?¤ì • ==========\n",
    "@dataclass\n",
    "class Config:\n",
    "    # ëª¨ë¸ ?¤ì •\n",
    "    model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"  \n",
    "    # model_name: str = \"meta-llama/Llama-2-7b-hf\"  # 7B ëª¨ë¸ (16GB ?´ìƒ)\n",
    "    \n",
    "    # ?°ì´??ê²½ë¡œ\n",
    "    train_data_path: str = \"HTP_data.jsonl\"\n",
    "    \n",
    "    # ì¶œë ¥ ê²½ë¡œ\n",
    "    output_dir: str = \"./htp_lora_model\"\n",
    "    \n",
    "    # ë°°ì¹˜ ?¬ê¸° (RTX 4060 8GB??\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # ?™ìŠµ ?¤ì •\n",
    "    num_train_epochs: int = 10\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.03\n",
    "    \n",
    "    # ?ìŠ¤??ê¸¸ì´\n",
    "    max_seq_length: int = 512\n",
    "    \n",
    "    # LoRA ?¤ì •\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    \n",
    "    # 8-bit ?‘ì??(ë©”ëª¨ë¦??ˆê°)\n",
    "    use_8bit: bool = True\n",
    "    \n",
    "    # ê¸°í?\n",
    "    seed: int = 42\n",
    "    fp16: bool = True  # Mixed precision training\n",
    "\n",
    "\n",
    "# ========== ?°ì´??ë¡œë”© ==========\n",
    "def load_jsonl_data(file_path: str) -> list:\n",
    "    \"\"\"JSONL ?Œì¼ ?½ê¸°\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_dataset(data: list, tokenizer, config: Config) -> Dataset:\n",
    "    \"\"\"?°ì´?°ì…‹ ì¤€ë¹?"\"\"\n",
    "    \n",
    "    def formatting_func(examples):\n",
    "        # inputê³?output???©ì³???™ìŠµ ?°ì´???ì„±\n",
    "        texts = []\n",
    "        for inp, out in zip(examples['input'], examples['output']):\n",
    "            text = f\"HTP ?´ì„ ?…ë ¥: {inp}\\n\\nHTP ?´ì„ ì¶œë ¥: {out}\"\n",
    "            texts.append(text)\n",
    "        \n",
    "        # ? í°??n",
    "        tokenized = tokenizer(\n",
    "            texts,\n",
    "            max_length=config.max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        # labels ?¤ì • (input_ids?€ ?™ì¼?˜ê²Œ)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    # Dataset?¼ë¡œ ë³€??n",
    "    dataset = Dataset.from_dict({\n",
    "        'input': [item['input'] for item in data],\n",
    "        'output': [item['output'] for item in data],\n",
    "    })\n",
    "    \n",
    "    # ì²˜ë¦¬\n",
    "    processed_dataset = dataset.map(\n",
    "        formatting_func,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        remove_columns=['input', 'output'],\n",
    "        desc=\"?¬ë§¤??ì¤?..\",\n",
    "    )\n",
    "    \n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "# ========== ëª¨ë¸ ë¡œë”© ë°?LoRA ?¤ì • ==========\n",
    "def setup_model_and_tokenizer(config: Config):\n",
    "    \"\"\"ëª¨ë¸ê³?? í¬?˜ì´?€ ?¤ì •\"\"\"\n",
    "    \n",
    "    print(f\"ëª¨ë¸ ë¡œë”©: {config.model_name}\")\n",
    "    \n",
    "    # ? í¬?˜ì´?€ ë¡œë”©\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë”© (8-bit ?‘ì??\n",
    "    if config.use_8bit:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            load_in_8bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        # 8-bit ?‘ì??ëª¨ë¸ ì¤€ë¹?n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model,\n",
    "            use_gradient_checkpointing=True,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    \n",
    "    # LoRA ?¤ì •\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],  # ëª¨ë¸???°ë¼ ?˜ì • ?„ìš”\n",
    "    )\n",
    "    \n",
    "    # LoRA ?ìš©\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # ?™ìŠµ ê°€?¥í•œ ?Œë¼ë¯¸í„° ?•ì¸\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d96d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ë©”ì¸ ?™ìŠµ ?¨ìˆ˜ ==========\n",
    "config = Config()\n",
    "\n",
    "# ?œë“œ ?¤ì •\n",
    "transformers.set_seed(config.seed)\n",
    "\n",
    "# ì¶œë ¥ ?”ë ‰? ë¦¬ ?ì„±\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. ?°ì´??ë¡œë“œ\n",
    "print(\"?°ì´??ë¡œë”© ì¤?..\")\n",
    "raw_data = load_jsonl_data(config.train_data_path)\n",
    "print(f\"ë¡œë“œ???˜í”Œ ?? {len(raw_data)}\")\n",
    "\n",
    "# 2. ëª¨ë¸, ? í¬?˜ì´?€ ?¤ì •\n",
    "model, tokenizer = setup_model_and_tokenizer(config)\n",
    "\n",
    "# 3. ?°ì´?°ì…‹ ì¤€ë¹?n",
    "print(\"?°ì´?°ì…‹ ì¤€ë¹?ì¤?..\")\n",
    "dataset = prepare_dataset(raw_data, tokenizer, config)\n",
    "\n",
    "# ?™ìŠµ/ê²€ì¦?ë¶„í•  (9:1)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=config.seed)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"?™ìŠµ ?˜í”Œ: {len(train_dataset)}, ê²€ì¦??˜í”Œ: {len(eval_dataset)}\")\n",
    "\n",
    "# 4. ?™ìŠµ ?¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # ë°°ì¹˜ ?¬ê¸°\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    \n",
    "    # ?í­\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    \n",
    "    # ?™ìŠµë¥?n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    \n",
    "    # ?µí‹°ë§ˆì´?€\n",
    "    optim=\"paged_adamw_32bit\",  # 8-bit ìµœì ??n",
    "    \n",
    "    # ?€??ë°?ë¡œê¹…\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    \n",
    "    # ê³„ì‚° ìµœì ??n",
    "    fp16=config.fp16,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # ê¸°í?\n",
    "    seed=config.seed,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# 5. Trainer ?¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8),\n",
    ")\n",
    "\n",
    "# 6. ?™ìŠµ ?œì‘\n",
    "print(\"?™ìŠµ ?œì‘...\")\n",
    "trainer.train()\n",
    "\n",
    "# 7. ëª¨ë¸ ?€??n",
    "print(\"ëª¨ë¸ ?€??ì¤?..\")\n",
    "model.save_pretrained(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "print(f\"???Œì¸?œë‹ ?„ë£Œ: {config.output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd119469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def check_device_placement(model):\n",
    "    \"\"\"ëª¨ë¸???¤ì œë¡??´ëŠ ?”ë°”?´ìŠ¤???ˆëŠ”ì§€ ?•ì¸\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"?” ?”ë°”?´ìŠ¤ ë°°ì¹˜ ?•ì¸\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    devices = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        device = str(param.device)\n",
    "        devices[device] = devices.get(device, 0) + 1\n",
    "    \n",
    "    for device, count in devices.items():\n",
    "        print(f\"  {device}: {count} ?Œë¼ë¯¸í„°\")\n",
    "    \n",
    "    print(f\"\\n??ì£??”ë°”?´ìŠ¤: {next(model.parameters()).device}\")\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "def load_finetuned_model_debug(model_path, base_model_name=None, use_4bit=False):\n",
    "    \"\"\"\n",
    "    ?”ë²„ê¹…ìš© ëª¨ë¸ ë¡œë“œ (4-bit ?‘ì??ê¸°ë³¸ OFF)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"?¨ê³„ 1: ?¤ì • ?•ì¸\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # CUDA ?¬ìš© ê°€???¬ë? ?•ì¸\n",
    "        print(f\"?”§ CUDA ?¬ìš© ê°€?? {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"?”§ GPU ?´ë¦„: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"?”§ GPU ë©”ëª¨ë¦? {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        # ?¤ì • ?Œì¼ ë¡œë“œ\n",
    "        adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        if os.path.exists(adapter_config_path):\n",
    "            with open(adapter_config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                base_model_name = config.get(\"base_model_name_or_path\", base_model_name)\n",
    "        \n",
    "        if base_model_name is None:\n",
    "            base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "        \n",
    "        print(f\"\\në² ì´??ëª¨ë¸: {base_model_name}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"?¨ê³„ 2: ëª¨ë¸ ë¡œë“œ (?‘ì???†ì´)\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # ?‘ì???†ì´ ë¡œë“œ (?”ë²„ê¹…ìš©)\n",
    "        if use_4bit:\n",
    "            print(\"? ï¸ 4-bit ?‘ì???¬ìš© (?ë¦´ ???ˆìŒ)\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"??FP16?¼ë¡œ ë¡œë“œ (ë¹ ë¦„)\")\n",
    "            quantization_config = None\n",
    "        \n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quantization_config,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"\\n??LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì¤?..\")\n",
    "        model = model.merge_and_unload()\n",
    "        \n",
    "        # ?”ë°”?´ìŠ¤ ?•ì¸\n",
    "        device = check_device_placement(model)\n",
    "        \n",
    "        print(\"\\n??ëª¨ë¸ ë¡œë“œ ?„ë£Œ!\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n??ëª¨ë¸ ë¡œë“œ ?¤íŒ¨: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def simple_generate(model, tokenizer, text, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    ìµœì†Œ?œì˜ ?¤ì •?¼ë¡œ ?ì„± ?ŒìŠ¤??n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"?§ª ê°„ë‹¨ ?ì„± ?ŒìŠ¤??")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ?…ë ¥ ì¤€ë¹?n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "    \n",
    "    print(f\"?…ë ¥ ?ìŠ¤?? {text}\")\n",
    "    print(f\"?…ë ¥ ? í° ?? {input_ids.shape[1]}\")\n",
    "    print(f\"?…ë ¥ ?”ë°”?´ìŠ¤: {input_ids.device}\")\n",
    "    print(f\"ëª¨ë¸ ?”ë°”?´ìŠ¤: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # ?ì„± ?œì‘\n",
    "    print(f\"\\n?±ï¸ ?ì„± ?œì‘... (max_new_tokens={max_new_tokens})\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # ë§¤ìš° ê°„ë‹¨???ì„± ?¤ì •\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # greedy decoding (ê°€??ë¹ ë¦„)\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"???ì„± ?„ë£Œ! ({elapsed:.2f}ì´?\")\n",
    "        \n",
    "        # ê²°ê³¼ ?”ì½”??n",
    "        generated_ids = outputs[0][input_ids.shape[1]:]\n",
    "        result = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"?ì„±??? í° ?? {len(generated_ids)}\")\n",
    "        print(f\"ì´ˆë‹¹ ? í°: {len(generated_ids)/elapsed:.2f} tokens/sec\")\n",
    "        print(f\"\\nê²°ê³¼:\\n{result}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"???ì„± ?¤íŒ¨ ({elapsed:.2f}ì´?ê²½ê³¼)\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def test_tokenizer(tokenizer):\n",
    "    \"\"\"? í¬?˜ì´?€ ?ŒìŠ¤??"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"?”¤ ? í¬?˜ì´?€ ?ŒìŠ¤??")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_texts = [\n",
    "        \"?ˆë…•?˜ì„¸??",\n",
    "        \"?˜ë¬´ê°€ ?¬ê³  ê°€ì§€ê°€ ë§ìœ¼ë©?ë¿Œë¦¬ê°€ ê¹Šê²Œ ?œí˜„??",\n",
    "        \"Hello world\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        tokens = tokenizer.encode(text)\n",
    "        print(f\"'{text}' ??{len(tokens)} ? í°\")\n",
    "    \n",
    "    print(f\"\\nPad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "    print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "\n",
    "# ë©”ì¸ ?¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"?? HTP ëª¨ë¸ ?”ë²„ê¹??œì‘\\n\")\n",
    "        \n",
    "        # 1?¨ê³„: ëª¨ë¸ ë¡œë“œ\n",
    "        model, tokenizer = load_finetuned_model_debug(\n",
    "            \"./htp_lora_model\",\n",
    "            base_model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "            use_4bit=False  # ?”ë²„ê¹…ì‹œ Falseë¡?\n",
    "        )\n",
    "        \n",
    "        # 2?¨ê³„: ? í¬?˜ì´?€ ?ŒìŠ¤??n",
    "        test_tokenizer(tokenizer)\n",
    "        \n",
    "        # 3?¨ê³„: ì´ˆê°„???ì„± ?ŒìŠ¤??(?ì–´)\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"?ŒìŠ¤??1: ?ì–´ ?ì„±\")\n",
    "        print(\"=\" * 70)\n",
    "        simple_generate(model, tokenizer, \"The tree is\", max_new_tokens=20)\n",
    "        \n",
    "        # 4?¨ê³„: ?œêµ­???ì„± ?ŒìŠ¤??n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"?ŒìŠ¤??2: ?œêµ­???ì„±\")\n",
    "        print(\"=\" * 70)\n",
    "        simple_generate(model, tokenizer, \"?˜ë¬´ê°€\", max_new_tokens=20)\n",
    "        \n",
    "        # 5?¨ê³„: ?„ë¡¬?„íŠ¸ ?•ì‹?¼ë¡œ ?ŒìŠ¤??n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"?ŒìŠ¤??3: Qwen ?„ë¡¬?„íŠ¸ ?•ì‹\")\n",
    "        print(\"=\" * 70)\n",
    "        qwen_prompt = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Say hello<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        simple_generate(model, tokenizer, qwen_prompt, max_new_tokens=20)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"??ëª¨ë“  ?ŒìŠ¤???„ë£Œ!\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n???ëŸ¬ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "def load_htp_model(model_path=\"./htp_lora_model\", base_model_name=None):\n",
    "    \"\"\"\n",
    "    HTP LoRA ëª¨ë¸ ë¡œë“œ\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"?? HTP ëª¨ë¸ ë¡œë“œ ì¤?..\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # CUDA ?•ì¸\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"??GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"??GPU ë©”ëª¨ë¦? {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\\n\")\n",
    "    else:\n",
    "        print(\"? ï¸ CPU ëª¨ë“œ (ë§¤ìš° ?ë¦´ ???ˆìŒ)\\n\")\n",
    "    \n",
    "    # ë² ì´??ëª¨ë¸ ?ë™ ?ì?\n",
    "    if base_model_name is None:\n",
    "        config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                base_model_name = config.get(\"base_model_name_or_path\", \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "        else:\n",
    "            base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    \n",
    "    print(f\"ë² ì´??ëª¨ë¸: {base_model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            dtype=torch.float16,  # torch_dtype ?€??dtype ?¬ìš©\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # LoRA ë³‘í•©\n",
    "        print(\"LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì¤?..\")\n",
    "        model = model.merge_and_unload()\n",
    "        \n",
    "        # ? í¬?˜ì´?€ ë¡œë“œ\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"??ëª¨ë¸ ë¡œë“œ ?„ë£Œ!\\n\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"???ëŸ¬: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def format_htp_prompt(htp_description, lang=\"ko\"):\n",
    "    \"\"\"\n",
    "    HTP ?´ì„???„ë¡¬?„íŠ¸ ?ì„±\n",
    "    \n",
    "    Args:\n",
    "        htp_description: HTP ê·¸ë¦¼ ?¹ì§• ?¤ëª…\n",
    "        lang: 'ko' (?œêµ­?? ?ëŠ” 'en' (?ì–´)\n",
    "    \"\"\"\n",
    "    if lang == \"ko\":\n",
    "        system_msg = \"\"\"?¹ì‹ ?€ HTP(ì§??˜ë¬´-?¬ëŒ) ê·¸ë¦¼ê²€???´ì„ ?„ë¬¸ ?¬ë¦¬?™ì?…ë‹ˆ??\n",
    "ì£¼ì–´ì§?ê·¸ë¦¼ ?¹ì§•??ë°”íƒ•?¼ë¡œ ?¬ë¦¬?™ì  ?´ì„???œê³µ?˜ì„¸??\n",
    "?´ì„?€ ëª…í™•?˜ê³  ?„ë¬¸?ì´?´ì•¼ ?˜ë©°, ê°€?¥í•œ ?¬ë¦¬???˜ë?ë¥??¤ëª…?´ì£¼?¸ìš”.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "{system_msg}<|im_end|>\n",
    "<|im_start|>user\n",
    "?¤ìŒ HTP ê·¸ë¦¼ ?¹ì§•???´ì„?´ì£¼?¸ìš”:\n",
    "{htp_description}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    else:\n",
    "        system_msg = \"\"\"You are a professional psychologist specialized in HTP (House-Tree-Person) test interpretation.\n",
    "Provide psychological interpretation based on the given drawing features.\n",
    "Your interpretation should be clear, professional, and explain possible psychological meanings.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "{system_msg}<|im_end|>\n",
    "<|im_start|>user\n",
    "Please interpret the following HTP drawing features:\n",
    "{htp_description}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def generate_htp_interpretation(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    htp_description,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    lang=\"ko\",\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    HTP ?´ì„ ?ì„±\n",
    "    \n",
    "    Args:\n",
    "        model: ë¡œë“œ??ëª¨ë¸\n",
    "        tokenizer: ? í¬?˜ì´?€\n",
    "        htp_description: HTP ê·¸ë¦¼ ?¹ì§• ?¤ëª…\n",
    "        max_new_tokens: ?ì„±??ìµœë? ? í° ??n",
    "        temperature: ?ì„± ?¤ì–‘??(?’ì„?˜ë¡ ì°½ì˜??\n",
    "        top_p: nucleus sampling\n",
    "        lang: 'ko' ?ëŠ” 'en'\n",
    "        verbose: ì§„í–‰ ?í™© ì¶œë ¥ ?¬ë?\n",
    "    \n",
    "    Returns:\n",
    "        str: ?ì„±???´ì„\n",
    "    \"\"\"\n",
    "    # ?„ë¡¬?„íŠ¸ ?ì„±\n",
    "    prompt = format_htp_prompt(htp_description, lang=lang)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"?“ HTP ?´ì„ ?ì„± ì¤?..\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"?…ë ¥: {htp_description}\\n\")\n",
    "    \n",
    "    # ? í°??n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # GPUë¡??´ë™\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"?…ë ¥ ? í° ?? {input_length}\")\n",
    "        print(f\"?ì„± ?œì‘... (ìµœë? {max_new_tokens} ? í°)\")\n",
    "    \n",
    "    # ?ì„±\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # ?”ì½”??(?…ë ¥ ?œì™¸)\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    interpretation = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"???ì„± ?„ë£Œ! ({elapsed:.2f}ì´?\")\n",
    "        print(f\"?ì„±??? í° ?? {len(generated_tokens)}\")\n",
    "        print(f\"?ë„: {len(generated_tokens)/elapsed:.1f} tokens/sec\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    return interpretation.strip()\n",
    "\n",
    "def batch_interpret(model, tokenizer, htp_list, **kwargs):\n",
    "    \"\"\"\n",
    "    ?¬ëŸ¬ HTP ?¤ëª…???œë²ˆ???´ì„\n",
    "    \n",
    "    Args:\n",
    "        model: ëª¨ë¸\n",
    "        tokenizer: ? í¬?˜ì´?€\n",
    "        htp_list: HTP ?¤ëª… ë¦¬ìŠ¤??n",
    "        **kwargs: generate_htp_interpretation???„ë‹¬??ì¶”ê? ?¸ì\n",
    "    \n",
    "    Returns:\n",
    "        list: ?´ì„ ê²°ê³¼ ë¦¬ìŠ¤??n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, htp_desc in enumerate(htp_list, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{i}/{len(htp_list)}] ?´ì„ ì¤?..\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        interpretation = generate_htp_interpretation(\n",
    "            model, tokenizer, htp_desc, **kwargs\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'input': htp_desc,\n",
    "            'interpretation': interpretation\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n?…ë ¥: {htp_desc}\")\n",
    "        print(f\"?´ì„: {interpretation}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ?¬ìš© ?ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. ëª¨ë¸ ë¡œë“œ\n",
    "        model, tokenizer = load_htp_model(\"./htp_lora_model\")\n",
    "        \n",
    "        \n",
    "        # 3. ë°°ì¹˜ ?´ì„ ?ŒìŠ¤??n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ë°°ì¹˜ ?´ì„ ?ŒìŠ¤??")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        test_cases = [\n",
    "            \"The tree is large with many branches and deeply drawn roots\",\n",
    "            \"The house is small with no doors or windows, and no smoke from the chimney\",\n",
    "            \"The person's hands are hidden behind their back and the face has no expression\",\n",
    "            \"The tree trunk has holes and the branches are drooping downward\",\n",
    "            \"The house has an excessively large roof with many complex decorations\",\n",
    "            \"The tree is drawn in the corner with very thin trunk and sparse leaves\",\n",
    "            \"The person is drawn very small at the bottom of the page with no facial features\",\n",
    "            \"The house has heavily shaded walls and windows with bars\",\n",
    "            \"The tree has broken branches and appears to be leaning to one side\",\n",
    "            \"The person has unusually large hands and feet with exaggerated details\"\n",
    "        ]\n",
    "        \n",
    "        results = batch_interpret(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            test_cases,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            lang=\"en\",  # ?ì–´ë¡?ë³€ê²?n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # 4. ê²°ê³¼ ?€??(? íƒ?¬í•­)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ê²°ê³¼ ?€??")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        output_file = \"htp_interpretations.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"??ê²°ê³¼ ?€?¥ë¨: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n???ëŸ¬ ë°œìƒ: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# ???¬ë°”ë¥?ë°©ë²• 1: ? í°??ì§ì ‘ ?…ë ¥\n",
    "token = \"your_huggingface_token_here\"  # ?¬ê¸°???¤ì œ ? í° ?…ë ¥\n",
    "\n",
    "# ?ëŠ”\n",
    "# ???¬ë°”ë¥?ë°©ë²• 2: ?˜ê²½ ë³€?˜ì—??ê°€?¸ì˜¤ê¸?(?˜ê²½ ë³€???´ë¦„?€ HF_TOKEN ê°™ì? ê²?\n",
    "# token = os.getenv(\"HF_TOKEN\")  # ?œìŠ¤???˜ê²½ ë³€?˜ì— HF_TOKEN=your_token ?¤ì • ?„ìš”\n",
    "\n",
    "api = HfApi(token=token)\n",
    "\n",
    "# ë¨¼ì? ?ˆí¬ê°€ ?ˆëŠ”ì§€ ?•ì¸?˜ê³  ?†ìœ¼ë©??ì„±\n",
    "try:\n",
    "    api.repo_info(repo_id=\"helena29/Qwen2.5_LoRA_for_HTP\", repo_type=\"model\")\n",
    "    print(\"???ˆí¬ì§€? ë¦¬ê°€ ?´ë? ì¡´ì¬?©ë‹ˆ??\")\n",
    "except:\n",
    "    print(\"?ˆí¬ì§€? ë¦¬ê°€ ?†ìŠµ?ˆë‹¤. ?ì„± ì¤?..\")\n",
    "    api.create_repo(\n",
    "        repo_id=\"helena29/Qwen2.5_LoRA_for_HTP\",\n",
    "        repo_type=\"model\",\n",
    "        private=False  # Trueë¡??¤ì •?˜ë©´ ë¹„ê³µê°?n",
    "    )\n",
    "    print(\"???ˆí¬ì§€? ë¦¬ ?ì„± ?„ë£Œ!\")\n",
    "\n",
    "# LoRA ëª¨ë¸ ?´ë”ë§??…ë¡œ??(?„ì²´ LoRa ?´ë”ê°€ ?„ë‹Œ ëª¨ë¸ ?´ë”ë§?\n",
    "print(\"\\n?…ë¡œ???œì‘...\")\n",
    "api.upload_folder(\n",
    "    folder_path=r\"C:\\Users\\helen\\Desktop\\kt cloud tech up\\basic_project\\models\\LoRa\\htp_lora_model\",  # ???˜ì •: ëª¨ë¸ ?´ë”ë§??…ë¡œ??n",
    "    repo_id=\"helena29/Qwen2.5_LoRA_for_HTP\",\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(\"???…ë¡œ???„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8514f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47567c25",
   "metadata": {},
   "source": [
    "# ?”„ ?„ì²´ ëª¨ë¸ ?…ë¡œ??(Base Model + LoRA ë³‘í•©)\n",
    "\n",
    "?„ì¬ HuggingFace?ëŠ” LoRA ?´ëŒ‘?°ë§Œ ?…ë¡œ?œëœ ?íƒœ?…ë‹ˆ??\n",
    "?„ë˜ ì½”ë“œ??Base Modelê³?LoRAë¥?ë³‘í•©???„ì²´ ëª¨ë¸???…ë¡œ?œí•©?ˆë‹¤.\n",
    "\n",
    "**ì£¼ì˜?¬í•­:**\n",
    "- ?„ì²´ ëª¨ë¸?€ ??3GB ?•ë„???¬ê¸°?…ë‹ˆ??n",
    "- ?…ë¡œ???œê°„???´ëŒ‘?°ë§Œ ?¬ë¦´ ?Œë³´???¤ë˜ ê±¸ë¦½?ˆë‹¤\n",
    "- ?¬ìš©?ëŠ” ??ëª¨ë¸??ë°”ë¡œ ?¬ìš©?????ˆìŠµ?ˆë‹¤ (LoRA ë¡œë”© ë¶ˆí•„??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# ========== ?¤ì • ==========\n",
    "LORA_MODEL_PATH = \"./htp_lora_model\"  # ë¡œì»¬ LoRA ëª¨ë¸ ê²½ë¡œ\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Base ëª¨ë¸\n",
    "MERGED_MODEL_PATH = \"./htp_merged_full_model\"  # ë³‘í•©??ëª¨ë¸ ?€??ê²½ë¡œ\n",
    "HF_REPO_NAME = \"helena29/Qwen2.5_LoRA_for_HTP_Full\"  # HuggingFace ?ˆí¬ ?´ë¦„ (?ˆë¡œ???ˆí¬)\n",
    "\n",
    "# HuggingFace ? í° (?¬ê¸°???¤ì œ ? í° ?…ë ¥)\n",
    "HF_TOKEN = \"your_huggingface_token_here\"  # ? ï¸ ?¤ì œ ? í°?¼ë¡œ ë³€ê²??„ìš”!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"?”„ ?„ì²´ ëª¨ë¸ ?ì„± ë°??…ë¡œ??")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1?¨ê³„: LoRA ëª¨ë¸ ë¡œë“œ ë°?ë³‘í•© ==========\n",
    "print(\"\\n[1/4] LoRA ëª¨ë¸ ë¡œë“œ ë°?Base Modelê³?ë³‘í•© ì¤?..\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "try:\n",
    "    # LoRA ëª¨ë¸ ë¡œë“œ\n",
    "    print(f\"LoRA ëª¨ë¸ ê²½ë¡œ: {LORA_MODEL_PATH}\")\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        LORA_MODEL_PATH,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"??LoRA ëª¨ë¸ ë¡œë“œ ?„ë£Œ\")\n",
    "    \n",
    "    # LoRAë¥?Base Model??ë³‘í•©\n",
    "    print(\"LoRA ê°€ì¤‘ì¹˜ë¥?Base Model??ë³‘í•© ì¤?..\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    print(\"??ë³‘í•© ?„ë£Œ!\")\n",
    "    \n",
    "    # ? í¬?˜ì´?€ ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"??? í¬?˜ì´?€ ë¡œë“œ ?„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"???ëŸ¬: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ========== 2?¨ê³„: ë³‘í•©??ëª¨ë¸ ë¡œì»¬ ?€??==========\n",
    "print(f\"\\n[2/4] ë³‘í•©???„ì²´ ëª¨ë¸??ë¡œì»¬???€??ì¤?..\")\n",
    "print(f\"?€??ê²½ë¡œ: {MERGED_MODEL_PATH}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "try:\n",
    "    os.makedirs(MERGED_MODEL_PATH, exist_ok=True)\n",
    "    \n",
    "    # ëª¨ë¸ ?€??n",
    "    merged_model.save_pretrained(\n",
    "        MERGED_MODEL_PATH,\n",
    "        safe_serialization=True,  # safetensors ?•ì‹?¼ë¡œ ?€??n",
    "        max_shard_size=\"2GB\"  # ?¤ë“œ ?¬ê¸° ?œí•œ\n",
    "    )\n",
    "    \n",
    "    # ? í¬?˜ì´?€ ?€??n",
    "    tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "    \n",
    "    print(\"??ë¡œì»¬ ?€???„ë£Œ!\")\n",
    "    \n",
    "    # ?€?¥ëœ ?Œì¼ ?•ì¸\n",
    "    saved_files = os.listdir(MERGED_MODEL_PATH)\n",
    "    print(f\"?€?¥ëœ ?Œì¼ ?? {len(saved_files)}\")\n",
    "    \n",
    "    # ëª¨ë¸ ?Œì¼ ?¬ê¸° ?•ì¸\n",
    "    total_size = 0\n",
    "    for file in saved_files:\n",
    "        file_path = os.path.join(MERGED_MODEL_PATH, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path) / (1024**3)  # GB\n",
    "            total_size += size\n",
    "            if size > 0.01:  # 10MB ?´ìƒ???Œì¼ë§?ì¶œë ¥\n",
    "                print(f\"  - {file}: {size:.2f} GB\")\n",
    "    \n",
    "    print(f\"?„ì²´ ?¬ê¸°: {total_size:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"???€???¤íŒ¨: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82816de9",
   "metadata": {},
   "source": [
    "## ?“Œ ì¶”ê? ?µì…˜: ê¸°ì¡´ ?ˆí¬????–´?°ê¸°\n",
    "\n",
    "ë§Œì•½ ???ˆí¬ê°€ ?„ë‹Œ ê¸°ì¡´ ?ˆí¬(`helena29/Qwen2.5_LoRA_for_HTP`)???„ì²´ ëª¨ë¸????–´?°ê³  ?¶ë‹¤ë©??„ë˜ ì½”ë“œë¥??¬ìš©?˜ì„¸??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# ========== ?¤ì • (ê¸°ì¡´ ?ˆí¬????–´?°ê¸°) ==========\n",
    "LORA_MODEL_PATH = \"./htp_lora_model\"\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "MERGED_MODEL_PATH = \"./htp_merged_full_model\"\n",
    "HF_REPO_NAME = \"helena29/Qwen2.5_LoRA_for_HTP\"  # ê¸°ì¡´ ?ˆí¬ ?´ë¦„\n",
    "\n",
    "# HuggingFace ? í°\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # ? ï¸ ?¤ì œ ? í°?¼ë¡œ ë³€ê²?\n",
    "\n",
    "print(\"? ï¸ ê¸°ì¡´ ?ˆí¬???„ì²´ ëª¨ë¸ ??–´?°ê¸° ëª¨ë“œ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. LoRA ë¡œë“œ ë°?ë³‘í•©\n",
    "print(\"[1/3] LoRA ëª¨ë¸ ë³‘í•© ì¤?..\")\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    LORA_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "merged_model = model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"??ë³‘í•© ?„ë£Œ\")\n",
    "\n",
    "# 2. ë¡œì»¬ ?€??n",
    "print(f\"\\n[2/3] ë¡œì»¬ ?€??ì¤? {MERGED_MODEL_PATH}\")\n",
    "os.makedirs(MERGED_MODEL_PATH, exist_ok=True)\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "print(\"???€???„ë£Œ\")\n",
    "\n",
    "# 3. ê¸°ì¡´ ?ˆí¬???…ë¡œ??(??–´?°ê¸°)\n",
    "print(f\"\\n[3/3] ê¸°ì¡´ ?ˆí¬???…ë¡œ??ì¤? {HF_REPO_NAME}\")\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "# ê¸°ì¡´ ?Œì¼ ?? œ ???…ë¡œ??n",
    "print(\"? ï¸ ê¸°ì¡´ ?Œì¼?¤ì„ ?? œ?˜ê³  ???Œì¼ë¡?êµì²´?©ë‹ˆ??..\")\n",
    "api.upload_folder(\n",
    "    folder_path=MERGED_MODEL_PATH,\n",
    "    repo_id=HF_REPO_NAME,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Replace adapter with full merged model\",\n",
    "    delete_patterns=[\"*.bin\", \"*.safetensors\", \"adapter_*\"]  # ê¸°ì¡´ ?´ëŒ‘???Œì¼ ?? œ\n",
    ")\n",
    "\n",
    "print(\"???…ë¡œ???„ë£Œ!\")\n",
    "print(f\"??URL: https://huggingface.co/{HF_REPO_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fb1b9",
   "metadata": {},
   "source": [
    "## ?”§ ë¬¸ì œ ?´ê²°: adapter_config.json ?Œì¼ ?…ë¡œ??n",
    "\n",
    "HuggingFace?ì„œ ?¤ìš´ë¡œë“œ ??`adapter_config.json` ?Œì¼???†ë‹¤???¤ë¥˜ê°€ ë°œìƒ?˜ë©´ ?„ë˜ ì½”ë“œë¡??„ë½???Œì¼?¤ì„ ?¤ì‹œ ?…ë¡œ?œí•˜?¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36369f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

